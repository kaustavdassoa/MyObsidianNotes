## **SUMMARY**

Agentic RAG (Retrieval-Augmented Generation), a framework that enhances traditional RAG systems by incorporating intelligent agents to handle complex tasks and make decisions dynamically.

**Traditional RAG Flow:**

- User query → Passed to Vector Database (containing company policies, documents, etc.) → Context retrieved and combined with prompt → Passed to LLM → Output generated
- LLM is used only once to generate the final response
    

**Agentic RAG Flow:**

- User query → Agent (powered by LLM) decides which vector database to query → Retrieves context from appropriate database → Passes to LLM with prompt → Output generated
- Agents act as intelligent routers that determine which retrieval source to use based on query intent
- Includes a "fail route" to handle queries the system cannot answer

The key difference is that in Agentic RAG, the LLM is converted into an agent that makes intelligent decisions about where to retrieve information from, rather than blindly querying a single source.

## **KEY TAKEAWAYS**

1. **Agentic RAG Definition** - It's a framework that enhances traditional RAG by incorporating intelligent agents to handle complex tasks and make decisions dynamically[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50013843#overview)​
2. **Agent as Router** - The agent uses LLM capabilities to route queries to the most relevant vector database based on query content and intent
3. **Multi-Database Architecture** - Organizations use multiple specialized vector databases (e.g., one for company policies, another for legal documents) rather than storing everything in one database
4. **Improved Accuracy** - By routing queries to domain-specific databases, Agentic RAG provides more accurate and relevant context to the LLM
5. **Graceful Failure Handling** - A "fail route" can be implemented to provide appropriate responses when queries fall outside the system's knowledge domain
    
6. **Agent-Based Decision Making** - The system checks relevance before retrieval, allowing the agent to decide whether to make a tool call or generate a response directly
    
7. **Use of Retrieval Tools** - To implement a retrieval agent, you simply need to give the LLM access to a retrieval tool connected to vector databases
    
8. **Iterative Processing** - The diagram shows that agents may repeat the retrieval and decision-making process multiple times until conditions are satisfied
    
9. **Traditional RAG Limitation** - Traditional RAG uses the LLM only once, making it less efficient when dealing with multiple knowledge sources
    
10. **Production Application** - This pattern is commonly used in enterprise chatbots and assistants that need to query multiple document sources intelligently
    

The next video will cover practical implementation of Agentic RAG using LangGraph.


# Lecture 121: Agentic RAG Implementation - Part 1 (22:41)

## SUMMARY

This hands-on coding lecture demonstrates the practical implementation of an Agentic RAG system using LangGraph and LangChain. The instructor builds a workflow with multiple vector databases and shows how to set up retrieval tools that can intelligently route queries to the appropriate data sources.

**Workflow Architecture:**

- Start node → Agent node → Retrieve node (dual vector databases) → Rewrite node (if context is poor) → Generate node → End
    
- Two vector databases: one containing LangGraph documentation blocks, another containing LangChain documentation blocks
    
- An agent with integrated LLM determines which database to query based on user query intent
    

**Implementation Steps Covered:**

1. **Environment Setup** - Import necessary libraries (os, dotenv, LangChain modules, OpenAI/Groq APIs)
    
2. **Create Vector Databases:**
    
    - Load URLs using WebBaseLoader (e.g., LangGraph and LangChain documentation URLs)
        
    - Extract text content from websites using document loaders
        
    - Split documents using RecursiveCharacterTextSplitter (chunk_size=1000, chunk_overlap=200)
        
    - Convert text to embeddings using OpenAI embeddings
        
    - Store embeddings in a local vector store using Chroma DB or Faiss
        
3. **Create Retrievers:**
    
    - Convert vector stores to retrievers using `vector_store.as_retriever()`
        
    - Test retriever with sample queries (e.g., "What is LangGraph?")
        
    - Each retriever returns relevant documents with metadata
        
4. **Create Retriever Tools:**
    
    - Use `create_retriever_tool()` from LangChain to wrap retrievers
        
    - Each tool has a name, description, and connected retriever
        
    - First tool: "retriever_langraph_db_search" - searches LangGraph documentation
        
    - Second tool: "retriever_langchain" - searches LangChain documentation
        
5. **Combine Tools:**
    
    - Merge all retriever tools into a tools list
        
    - These tools will be bound with LLM to create the agent
        

**Key Code Concepts:**

- WebBaseLoader for URL content extraction
    
- RecursiveCharacterTextSplitter for document chunking
    
- OpenAI embeddings for vector conversion
    
- Retriever tools for LLM integration
    

**Next Steps (covered in Part 2):**

- Create agent nodes and functions
    
- Build retrieve, rewrite, and generate nodes
    
- Implement the complete LangGraph workflow
    
- Bind tools with LLM to enable intelligent routing
    

[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50016061#overview)​

## KEY TAKEAWAYS

1. **Multi-Vector Database Architecture** - Organize knowledge across specialized vector databases for domain-specific retrieval (e.g., LangGraph docs in one DB, LangChain docs in another)
    
2. **WebBaseLoader Integration** - Use LangChain's WebBaseLoader to programmatically fetch and process content from URLs, enabling dynamic knowledge base creation
    
3. **Document Chunking** - Split large documents into smaller chunks (chunk_size=1000, overlap=200) to improve relevance and reduce token usage during embedding
    
4. **Embedding Pipeline** - Text → Embeddings (via OpenAI) → Vector Store (Chroma/Faiss) → Retriever enables semantic search capabilities
    
5. **Retriever to Tool Conversion** - Convert retrievers to tools using `create_retriever_tool()` to make them compatible with LLM agent systems
    
6. **Tool Metadata** - Each tool requires a clear name and description so the LLM can understand its purpose and decide when to use it
    
7. **Test Queries** - Use `.invoke()` on retrievers with test queries to verify they're retrieving relevant context before integration
    
8. **Tool Composition** - Combine multiple retrieval tools into a single tools list that will be bound with the LLM agent
    
9. **Semantic Search** - LLM-powered agents can route "What is LangGraph?" to LangGraph docs and "What is LangChain?" to LangChain docs based on semantic understanding
    
10. **Modular Implementation** - Build the system incrementally: setup → load data → embed → retrieve → create tools → (coming next) create agents
    
11. **Local Vector Storage** - Using local vector stores (Chroma/Faiss) with CPU-based setup for development without GPU requirements
    
12. **Dependency Management** - Install required packages (e.g., `faiss-cpu` for CPU-only environments) before executing code to avoid runtime errors
    

This is the foundational step for building intelligent retrieval-augmented agents that can dynamically choose information sources based on query intent.

# Lecture 123: Corrective RAG Theoretical Understanding (7:13)

## SUMMARY

This lecture introduces Corrective RAG (CRAG), an advanced RAG technique that improves accuracy and relevance of generated responses by incorporating quality evaluation and corrective mechanisms. Unlike basic RAG which retrieves and generates, Corrective RAG adds a grading/evaluation layer that validates retrieval quality and triggers corrective actions when necessary.

**Core Definition:**  
Corrective RAG is an advanced technique within RAG that focuses on improving accuracy and relevance of generated responses by incorporating mechanisms for **self-reflection and self-grading** of retrieved documents.[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50136759#overview)​

**Workflow Architecture:**

- User Question → Retriever Node (fetch from vector DB) → Grade Node (evaluate relevance) → Decision Point:
    
    - **If Relevant (Yes)** → Generate Answer
        
    - **If Irrelevant (No)** → Apply Corrective Actions (Rewrite Query + Web Search) → Generate Answer
        

**Key Components:**

1. **Retriever** - Fetches candidate documents from vector database
    
2. **Grader** - Evaluates whether retrieved documents are relevant to the query using an LLM
    
3. **Refinement & Correction** - Applies corrective actions when documents are irrelevant
    
4. **Web Search** - External search used as a fallback retrieval mechanism
    

**Self-Reflection and Self-Grading Explained:**

- **Self-Reflection:** The system evaluates the quality of its own retrievals
    
- **Self-Grading:** The system applies a rating/judgment to determine if retrieved content is relevant
    
- When documents are graded as irrelevant, the system takes corrective actions instead of generating from poor context
    

**Corrective Actions:**  
When retrieved documents are deemed irrelevant:

- Rewrite the original user query to improve clarity
    
- Perform web search to find alternative sources of information
    
- Combine both refined query results with web search results
    
- Then generate the final answer with higher-quality context
    

[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50136759#overview)​

## KEY TAKEAWAYS

1. **Quality Control in RAG** - Corrective RAG adds an evaluation layer (grader node) that traditional RAG lacks, ensuring retrieved documents are actually relevant before generation
    
2. **Self-Grading Mechanism** - An LLM-powered grader evaluates retrieved documents and returns binary judgment (relevant/irrelevant), enabling intelligent decision-making
    
3. **Fallback Strategy** - When vector DB retrieval is insufficient, web search acts as a corrective fallback mechanism to find alternative information sources
    
4. **Query Rewriting** - Instead of working with potentially flawed user queries, Corrective RAG can rewrite queries for better retrieval when initial attempts fail
    
5. **Improved Accuracy** - By filtering irrelevant documents before generation, Corrective RAG ensures more accurate and relevant LLM responses compared to basic RAG
    
6. **Increased Robustness** - The system can identify and filter out irrelevant information, making it more robust to incomplete or noisy data in vector databases
    
7. **Multi-Stage Evaluation** - Unlike single-pass RAG systems, Corrective RAG employs multiple evaluation gates (grading) before final response generation
    
8. **Relevance Preservation** - Self-reflection prevents hallucinations caused by using irrelevant retrieved content in LLM prompts
    
9. **Hybrid Retrieval** - Combines vector database retrieval with web search, giving the system multiple knowledge acquisition pathways
    
10. **Corrective Actions are Flexible** - While query rewriting and web search are shown, corrective actions can be customized based on application requirements
    
11. **Interview-Ready Concept** - Understanding the distinction between basic RAG (retrieve → generate) and Corrective RAG (retrieve → grade → correct if needed → generate) is valuable for technical interviews
    
12. **Production-Ready Quality** - This approach is suitable for enterprise applications where accuracy of retrieved context is critical
    

**Next Implementation:** The practical implementation using LangGraph will involve creating nodes for retriever, grader, rewriter, and web search, connected through a state graph that manages the decision flow.



# Lecture 124: Corrective RAG Practical Implementation (13:55)

## SUMMARY

This lecture demonstrates the complete end-to-end implementation of Corrective RAG using LangGraph and LangChain. The instructor builds a functional workflow that retrieves documents, grades their relevance, and applies corrective actions (query rewriting and web search) when documents are irrelevant before generating final answers.[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50136781#overview)​

**Implementation Architecture:**

The workflow follows this graph structure:

- **Retriever Node** → Retrieves documents from vector database
    
- **Grade Documents Node** → Uses LLM to evaluate if retrieved documents are relevant (binary yes/no)
    
- **Conditional Edge** → Decision point based on relevance score
    
    - If "Yes" (Relevant) → Generate answer
        
    - If "No" (Irrelevant) → Transform Query → Web Search → Generate answer
        

**Step-by-Step Implementation:**

**1. Data Preparation:**

- Load URLs (blogs on prompt engineering, adversarial attacks, and agents)
    
- Use WebBaseLoader to fetch content from URLs
    
- Apply RecursiveCharacterTextSplitter to chunk documents
    
- Create embeddings using OpenAI embeddings
    
- Store in vector database and create retriever
    

**2. Retriever Grader Creation:**

- Import ChatOpenAI for structured outputs
    
- Use Pydantic's BaseModel to define `GradeDocument` data model with `binary_score` field (yes/no)
    
- Create system prompt: "You are a grader assessing relevance of retrieved documents to user questions"
    
- Use `with_structured_output()` to ensure LLM returns binary scores
    
- Chain the prompt with LLM to create retriever_grader
    

**3. Generate Node:**

- Use hub.pull() to fetch pre-built RAG prompt from LangChain Hub
    
- Create chain: Prompt → LLM (GPT-3.5 turbo) → StringOutputParser
    
- Combine retrieved documents as context with user question
    
- Generate comprehensive answers based on relevant context
    

**4. Question Rewriter:**

- Create system prompt for rewriting poorly-matched queries
    
- Use LLM with structured output for consistent reformulation
    
- Example: "agent memory" → "What is the role of memory in artificial intelligence agents?"
    
- Improves retrieval accuracy for irrelevant document cases
    

**5. Web Search Node:**

- Use Tavily search API for web search integration
    
- Set k=3 to retrieve top 3 results
    
- Converts search results into Document objects
    
- Acts as fallback when vector database retrieval is insufficient
    

**6. State Graph Construction:**

- Define GraphState with fields: question, generation, web_search, documents
    
- Create node functions for each workflow step
    
- Define edges and conditional edges connecting nodes
    
- Implement decide_to_generate() function to determine flow path
    

**7. Full Workflow Execution:**

- Example query: "What are the types of agent memory?"
    
- System checks document relevance
    
- If irrelevant: transforms query, performs web search, retrieves results
    
- Combines retrieved and web search documents
    
- Generates accurate, comprehensive answer with metadata
    

[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50136781#overview)​

## KEY TAKEAWAYS

1. **Structured Output with Pydantic** - Using Pydantic BaseModel with LLM's `with_structured_output()` ensures binary yes/no responses from grader, enabling reliable conditional logic
    
2. **Retriever-Grader-Generate Pipeline** - The three-stage pipeline (retrieve → grade → generate) ensures only relevant context feeds into LLM, reducing hallucinations
    
3. **Binary Scoring for Relevance** - Grader returns yes/no decisions instead of confidence scores, making conditional edge logic straightforward and deterministic
    
4. **Conditional Edges in LangGraph** - Conditional edges enable intelligent branching: if documents are irrelevant, trigger query rewriting and web search; otherwise generate directly
    
5. **Query Rewriting Strategy** - Automatically rewrite user queries when initial retrieval fails, improving clarity and semantic alignment with available knowledge
    
6. **Web Search as Fallback** - Tavily Search API integration provides external information when vector database lacks relevant content, ensuring comprehensive coverage
    
7. **State Management Across Nodes** - GraphState maintains question, documents, generation, and web_search fields, allowing information flow between nodes throughout workflow
    
8. **LangChain Hub for Prompts** - Using `hub.pull()` for pre-built RAG prompts provides production-tested system prompts without manual engineering
    
9. **Document Conversion** - Web search results must be converted to Document objects with page_content and metadata for consistency in processing pipeline
    
10. **Node Function Definition** - Each node function takes state as input, performs computation, and returns updated state fields for downstream nodes to use
    
11. **Metadata Preservation** - Documents maintain source metadata (URLs, titles) through retrieval and web search, enabling transparency and citation tracking
    
12. **Graph Visualization and Testing** - Draw the graph structure to understand flow, then test with sample queries to verify conditional branching works correctly
    
13. **Collective Rank Concept** - The combination of retriever documents + web search results + grading creates "collective rank" - leveraging multiple information sources for better accuracy
    
14. **Fast Implementation** - With proper understanding of components (retriever, grader, generator, query rewriter, web search), full workflows can be built quickly by combining existing tools
    
15. **Production-Ready Pattern** - This implementation pattern is suitable for enterprise applications requiring high accuracy and robustness against incomplete or noisy vector databases
    

**Next Steps:** Adaptive RAG (coming next) combines Corrective RAG with routing mechanisms to automatically select retrieval strategies based on query complexity.


# Lecture 125: Adaptive RAG Theoretical Understanding (11:54)

## SUMMARY

This lecture introduces Adaptive RAG, an intelligent retrieval-augmented generation framework that dynamically adjusts its retrieval strategy based on query complexity. Unlike previous RAG approaches that use fixed strategies, Adaptive RAG intelligently routes queries to appropriate retrieval methods and applies self-corrective mechanisms for improved accuracy.[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50038613#overview)​

**Core Definition:**  
Adaptive RAG (Retrieval-Augmented Generation) is a framework that dynamically adjusts its strategy for handling queries based on their complexity. It acts like a smart assistant that knows when to dig deep for information and when to provide simple answers, instead of using a single rigid approach. Adaptive RAG chooses the most appropriate retrieval method for each query, balancing speed and accuracy.[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50038613#overview)​

**Two Main Components:**

1. **Query Analysis** - Intelligently classifies query complexity to route to appropriate retrieval method
    
2. **RAG + Self-Reflection** (also called Self-Corrective RAG) - Evaluates and refines retrieved content through iterative grading and query rewriting
    

**Query Analysis - Routing Strategy:**  
The system uses a classifier to determine which retrieval path to follow based on query complexity:

- **Simple Queries** (e.g., "What is the capital of India?") → Direct LLM response without external retrieval
    
- **Moderately Complex Queries** (e.g., "Economics of India") → Web search for recent/current information
    
- **Highly Complex Queries** (e.g., "Company X policies in India") → Vector database retrieval with self-reflection and query rewriting
    

**RAG + Self-Reflection Workflow:**  
After query analysis directs the query to appropriate retrieval source:

1. **Retrieve** documents from selected source (web search or vector database)
    
2. **Grade Documents** - Check if retrieved content is relevant to the query
    
3. **Check for Hallucination** - Validate that generated answers don't contradict the retrieved context
    
4. **Verify Accuracy** - Compare answer against retrieved context for accuracy
    
5. **Iterative Refinement** - If documents/answers are not satisfactory:
    
    - Rewrite the query for better semantic alignment
        
    - Send back to retriever for fresh attempt
        
    - Repeat until satisfactory results obtained
        

**Example Workflow:**  
Query: "What are the types of agent memory?"

- Query Analysis: Classifies as complex → Routes to Retriever
    
- Retrieve: Gets documents from vector database
    
- Grade: Validates relevance of retrieved documents
    
- If relevant → Generate answer
    
- If irrelevant → Transform query → Retrieve again → Grade again → Generate answer
    
- Each iteration includes hallucination and accuracy checks
    

**Key Difference from Agentic RAG:**

- Agentic RAG: Agent makes routing decisions based on available routes
    
- Adaptive RAG: Classifier makes routing decisions based on query complexity, with built-in self-correction mechanisms
    

[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50038613#overview)​

## KEY TAKEAWAYS

1. **Query Complexity Classification** - The foundation of Adaptive RAG is a classifier that analyzes query complexity to determine the most efficient retrieval strategy, not blindly using all methods
    
2. **Three-Tier Routing Strategy** - Simple queries skip external retrieval entirely (direct LLM), moderate queries use web search, complex queries use specialized vector databases with self-reflection
    
3. **Self-Reflection Loop** - Unlike previous RAG methods, Adaptive RAG includes feedback loops where the system validates its own output and iteratively improves through query rewriting and re-retrieval
    
4. **Hallucination Detection** - The system includes checks to detect when LLM generates contradictory information compared to retrieved context, triggering regeneration or query refinement
    
5. **Accuracy Verification** - Answer accuracy is verified by comparing generated responses against retrieved documents, catching misalignments before returning results to users
    
6. **Iterative Refinement** - Failed retrievals don't immediately trigger fallback searches; instead, the query is rewritten for semantic clarity and the same source is queried again
    
7. **Balancing Speed and Accuracy** - By routing simple queries directly to LLM and complex queries through retrieval loops, the system optimizes response latency while maintaining accuracy
    
8. **Multi-Path Architecture** - The system is flexible and allows adding custom retrieval paths (internal DB, web search, APIs) beyond standard options
    
9. **Query-Driven Adaptivity** - The system's behavior changes dynamically based on input characteristics, not fixed configuration—what works for one query may not work for another
    
10. **Self-Corrective RAG** - The self-reflection component enables the system to detect and correct its own errors, reducing hallucinations and improving response quality through multiple validation stages
    
11. **Conditional Edges Pattern** - LangGraph's conditional edge feature is crucial for implementing branching logic based on query complexity and validation results at each stage
    
12. **Comparison to Previous Approaches** - Combines strengths of Agentic RAG (intelligent routing) with Corrective RAG (self-grading and query rewriting) into a unified adaptive system
    
13. **Real-World Applicability** - Particularly useful for systems handling heterogeneous queries (general knowledge, current events, proprietary data) where one-size-fits-all approaches fail
    
14. **Interview Preparation** - Understanding the progression from basic RAG → Agentic RAG → Corrective RAG → Adaptive RAG demonstrates mastery of modern RAG architectures
    

**Implementation Preview:** The next lecture will implement this complete workflow using LangGraph, including query analysis classifier, conditional routing, retriever graders, and iterative refinement loops.


I've successfully extracted and analyzed the transcript from Udemy lecture 126 "Adaptive RAG Implementation" (17:02 minutes) from the Complete Agentic AI Bootcamp course.

## SUMMARY

This lecture provides a complete end-to-end implementation of Adaptive RAG using LangGraph and LangChain. The instructor builds individual components step-by-step including a query router that classifies questions into "vector_store" or "web_search" categories, document relevance graders, answer generators with hallucination detection, and query rewriters. These components are then orchestrated using LangGraph's state management and conditional edges to create an intelligent, self-correcting RAG system.[](https://www.udemy.com/course/complete-agentic-ai-bootcamp-with-langgraph-and-langchain/learn/lecture/50038981#overview)​

**Key Architecture Components:**

- Query Router: Routes queries based on complexity to appropriate retrieval source
    
- Retrieval Grader: Validates relevance of retrieved documents
    
- Generator: Creates answers from retrieved context
    
- Hallucination Checker: Detects contradictions between generated answers and source documents
    
- Answer Grader: Verifies answers address the user's question
    
- Question Rewriter: Improves queries for better semantic alignment
    
- Web Search Integration: Tavily API for fallback retrieval
    

**Example Workflows:**

- Simple query ("What is machine learning?") → Web Search → Generate → Verify → Output
    
- Complex query ("What are types of agent memory?") → Vector Retrieval → Grade → Generate → Hallucination Check → Answer Verification → Output
    

## KEY TAKEAWAYS

1. **Structured Output for Classification** - Pydantic models with LLM's `with_structured_output()` ensure reliable query routing
    
2. **Modular Node Design** - Build and test components independently before connecting in the state graph
    
3. **Multiple Quality Gates** - Document relevance, hallucination detection, and answer quality checks prevent errors throughout the pipeline
    
4. **Early Query Analysis** - Classify queries upfront to avoid unnecessary complex processing for simple questions
    
5. **Hybrid Retrieval Strategy** - Intelligently combine vector databases for indexed knowledge with web search for current information
    
6. **Iterative Self-Correction** - Failed quality checks trigger query rewriting and re-retrieval rather than immediate fallback
    
7. **State Graph Orchestration** - LangGraph manages information flow across nodes with conditional edges enabling complex branching logic
    
8. **Binary Scoring for Determinism** - Yes/no decisions at each checkpoint enable reliable automated decision-making
    
9. **LLM-Powered Intelligence** - Every critical decision point leverages structured LLM outputs for intelligent routing and validation
    
10. **Hub Prompts for Reliability** - Using pre-tested RAG prompts from LangChain Hub improves answer quality
    
11. **Metadata Preservation** - Source tracking enables transparency and citation in generated responses
    
12. **Production-Ready Pattern** - This architecture handles heterogeneous queries across multiple knowledge sources efficiently
    

The instructor emphasizes visually following the workflow graph to understand how different queries take different paths through the system, with each path including appropriate validation mechanisms for robust, accurate responses.