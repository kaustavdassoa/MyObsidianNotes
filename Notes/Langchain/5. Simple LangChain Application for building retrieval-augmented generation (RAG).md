## Overview

**Retrieval-Augmented Generation (RAG)** system built with LangChain that combines vector stores, embeddings, and language models to retrieve and process contextual information. The implementation uses Chroma as the vector database backend and HuggingFace embeddings for semantic understanding.
## Key Concepts:

1. **LangChain Document Abstraction capability** : LangChain implements a document abstraction with two main attributes **page_content**, and **metadata** .Documents can represent chunks of larger documents example individual pages from a PDF.
	- `page_content`: A string representing the actual text content
	- `metadata`: A dictionary containing arbitrary metadata about the document (source, page number, etc.)
2. **Embedding** - Embedding Technique is leveraged to convert the document chunks into vector, leveraging HuggingFace embeddings, Leveraging `all-MiniLM-L6-v2` mode for this purpose :
	- Maps sentences and paragraphs to a 384-dimensional dense vector space
	- Can be used for clustering and semantic search
	- Is open-source and can be downloaded locally from HuggingFace
3. **Vector Stores**** : Vector stores convert text into embeddings (word vectors) and store them in a (vector) database. example Chroma DB
	- Converting text documents into vector representations using embedding techniques
	- Storing these vectors in the vector store database
	- Enabling similarity search on the stored vectors
4. **Vector Store to Retriever Conversion**  : While vector stores are not subclasses of "Runnable" (LangChain's standard interface), retrievers are. There are two possible ways to use Vector Store for similarity Search 
	- **Method 1: Using RunnableLambda** - Creates a custom runnable function that wraps the similarity search functionality
	- **Method 2: Using `as_retriever()`** - The preferred method that converts the vector store into a proper retriever with configurable search parameters (search_type, search_kwargs)   
5. **Building a Complete RAG Chain** : Leveraging the retrieved context in the LLM query 
	- Create a chat prompt template with context and question placeholders 
	- Use `RunnablePassThrough` to pass user input to the chain
	- Integrate the retriever to fetch context from vector store
	- Chain the components together: input → retriever (context) + prompt + LLM → response
6. Display Response : Print Response 

## Key Takeaways

1. **Document Structure**: LangChain documents have two components—`page_content` (the actual text) and `metadata` (associated information)—allowing structured representation of chunks from larger documents.
2. **Vector Embeddings**: Text is converted to high-dimensional vectors (384 dimensions in the example) that capture semantic meaning, enabling similarity-based search and retrieval.
3. **Runnable Abstraction**: Retrievers are "Runnable" objects that can be directly integrated into LangChain Expression Language (LCEL) chains, unlike raw vector stores.
4. **Two Conversion Methods**: Vector stores can be converted to retrievers using either `RunnableLambda` (custom approach) or the `as_retriever()` method (recommended, provides better control).
5. **Open-Source Flexibility**: The lecture demonstrates using entirely open-source tools (HuggingFace embeddings, Groq models, Chroma) with proper credential management through environment variables.
6. **Similarity Scoring**: Vector stores support similarity search with score reporting, allowing quantification of match relevance (lower scores indicate better matches).
7. **Batch Operations**: Both vector stores and retrievers support batch operations for processing multiple queries simultaneously.
8. **RAG Chain Architecture**: A complete RAG system combines: documents → embeddings → vector store → retriever → prompt template → LLM → final response.
9. **Search Configuration**: Retrievers can be customized with search parameters like `search_type` (similarity) and `search_kwargs` (top-k results) for fine-tuned retrieval behavior.
10. **Practical Integration**: The retriever acts as an interface between the vector store and the LCEL chain, enabling seamless data flow from stored documents to LLM prompts.
11. **Model Selection**: The `all-MiniLM-L6-v2` model is suitable for general semantic search tasks and can be downloaded once then cached locally for reuse.
12. **Environment Management**: Proper setup of requirement.txt files and environment variables is essential for managing API keys and model downloads across different virtual environments.